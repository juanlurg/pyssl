{
 "cells": [
  {
   "cell_type": "code",
   "source": "# üöÄ Setup for Google Colab\nimport sys\nif 'google.colab' in sys.modules:\n    print(\"üîß Setting up for Google Colab...\")\n    \n    # Install required dependencies\n    !pip install -q matplotlib seaborn scikit-learn numpy pandas\n    \n    # Note: SSL framework code will be included in subsequent cells for Colab compatibility\n    print(\"‚úÖ Dependencies installed! SSL framework will be defined in the next cells.\")\nelse:\n    print(\"üìù Running locally - using installed SSL framework\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/pyssl/blob/main/notebooks/02_classification_comparison.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ SSL Strategy Comparison - Find the Best Approach\n",
    "\n",
    "This notebook compares different semi-supervised learning strategies to help you choose the best approach for your data.\n",
    "\n",
    "**What you'll learn:**\n",
    "- When to use `ConfidenceThreshold` vs `TopKFixedCount` strategies\n",
    "- How different integration methods affect performance\n",
    "- How to handle imbalanced datasets with SSL\n",
    "- Which strategy works best for your use case\n",
    "\n",
    "**Dataset:** Imbalanced 3-class problem (simulating real-world scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our SSL framework\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ssl_framework.main import SelfTrainingClassifier\n",
    "from ssl_framework.strategies import (\n",
    "    ConfidenceThreshold, TopKFixedCount, \n",
    "    AppendAndGrow, FullReLabeling, ConfidenceWeighting\n",
    ")\n",
    "\n",
    "# Import our utilities\n",
    "from utils.data_generation import make_imbalanced_classification, generate_ssl_dataset\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Challenging Imbalanced Dataset\n",
    "\n",
    "We'll create a 3-class imbalanced dataset that mimics real-world scenarios where SSL is most beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create imbalanced dataset\nX, y = make_imbalanced_classification(\n    n_samples=2000,\n    n_features=10,\n    n_classes=3,\n    weights=[0.1, 0.3, 0.6],  # Highly imbalanced: 10%, 30%, 60%\n    n_informative=6,\n    n_redundant=2,\n    class_sep=0.7,  # Moderate separation - challenging but learnable\n    random_state=42\n)\n\n# Apply our standard SSL splits using the custom split function\nfrom utils.data_generation import _apply_custom_splits\nX_labeled, y_labeled, X_unlabeled, X_val, y_val, X_test, y_test, y_unlabeled_true = _apply_custom_splits(\n    X, y, n_labeled=60, test_size=0.2, val_size=0.15, random_state=42\n)\n\nprint(f\"üìä Dataset Statistics:\")\nprint(f\"   Total samples: {len(X)}\")\nprint(f\"   Labeled: {len(X_labeled)}, Unlabeled: {len(X_unlabeled)}\")\nprint(f\"   Validation: {len(X_val)}, Test: {len(X_test)}\")\nprint(f\"   Features: {X.shape[1]}\")\n\nprint(f\"\\nüéØ Class Distribution in Full Dataset:\")\nclass_counts = np.bincount(y)\nfor i, count in enumerate(class_counts):\n    print(f\"   Class {i}: {count} samples ({count/len(y)*100:.1f}%)\")\n\nprint(f\"\\nüè∑Ô∏è Class Distribution in Labeled Set:\")\nlabeled_counts = np.bincount(y_labeled)\nfor i, count in enumerate(labeled_counts):\n    print(f\"   Class {i}: {count} samples ({count/len(y_labeled)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Experiment Framework\n",
    "\n",
    "Let's create a systematic way to test different strategies and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ssl_experiment(strategy_name, selection_strategy, integration_strategy, base_model):\n",
    "    \"\"\"\n",
    "    Run a single SSL experiment with given strategies.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Running {strategy_name}...\")\n",
    "    \n",
    "    # Create SSL model\n",
    "    ssl_model = SelfTrainingClassifier(\n",
    "        base_model=base_model,\n",
    "        selection_strategy=selection_strategy,\n",
    "        integration_strategy=integration_strategy,\n",
    "        max_iter=15,\n",
    "        labeling_convergence_threshold=5\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    ssl_model.fit(X_labeled, y_labeled, X_unlabeled, X_val, y_val)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = ssl_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'strategy': strategy_name,\n",
    "        'model': ssl_model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'predictions': y_pred,\n",
    "        'history': ssl_model.history_,\n",
    "        'stopping_reason': ssl_model.stopping_reason_\n",
    "    }\n",
    "\n",
    "def run_baseline(base_model):\n",
    "    \"\"\"\n",
    "    Run baseline supervised learning experiment.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Running Baseline (Supervised Only)...\")\n",
    "    \n",
    "    # Train on labeled data only\n",
    "    baseline_model = base_model\n",
    "    baseline_model.fit(X_labeled, y_labeled)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = baseline_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'strategy': 'Baseline (Supervised)',\n",
    "        'model': baseline_model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'predictions': y_pred,\n",
    "        'history': [],\n",
    "        'stopping_reason': 'N/A'\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Experiment framework ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strategy Comparison Experiments\n",
    "\n",
    "Let's test different combinations of selection and integration strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base model (we'll use LogisticRegression for consistency)\n",
    "base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Define experiments to run\n",
    "experiments = [\n",
    "    # Baseline\n",
    "    ('Baseline', None, None),\n",
    "    \n",
    "    # Confidence Threshold strategies\n",
    "    ('Confidence-0.95 + AppendGrow', \n",
    "     ConfidenceThreshold(threshold=0.95), \n",
    "     AppendAndGrow()),\n",
    "    \n",
    "    ('Confidence-0.90 + AppendGrow', \n",
    "     ConfidenceThreshold(threshold=0.90), \n",
    "     AppendAndGrow()),\n",
    "    \n",
    "    ('Confidence-0.85 + AppendGrow', \n",
    "     ConfidenceThreshold(threshold=0.85), \n",
    "     AppendAndGrow()),\n",
    "    \n",
    "    # TopK strategies\n",
    "    ('TopK-5 + AppendGrow', \n",
    "     TopKFixedCount(k=5), \n",
    "     AppendAndGrow()),\n",
    "    \n",
    "    ('TopK-10 + AppendGrow', \n",
    "     TopKFixedCount(k=10), \n",
    "     AppendAndGrow()),\n",
    "    \n",
    "    ('TopK-15 + AppendGrow', \n",
    "     TopKFixedCount(k=15), \n",
    "     AppendAndGrow()),\n",
    "    \n",
    "    # Different integration strategies\n",
    "    ('Confidence-0.90 + FullReLabel', \n",
    "     ConfidenceThreshold(threshold=0.90), \n",
    "     FullReLabeling(X_labeled, y_labeled)),\n",
    "    \n",
    "    ('TopK-10 + ConfWeight', \n",
    "     TopKFixedCount(k=10), \n",
    "     ConfidenceWeighting()),\n",
    "]\n",
    "\n",
    "# Run all experiments\n",
    "results = []\n",
    "\n",
    "for exp_name, selection_strategy, integration_strategy in experiments:\n",
    "    if exp_name == 'Baseline':\n",
    "        result = run_baseline(LogisticRegression(random_state=42, max_iter=1000))\n",
    "    else:\n",
    "        result = run_ssl_experiment(\n",
    "            exp_name, \n",
    "            selection_strategy, \n",
    "            integration_strategy, \n",
    "            LogisticRegression(random_state=42, max_iter=1000)\n",
    "        )\n",
    "    results.append(result)\n",
    "    print(f\"   ‚úÖ {exp_name}: Accuracy = {result['accuracy']:.3f}, F1-Macro = {result['f1_macro']:.3f}\")\n",
    "\n",
    "print(\"\\nüèÜ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Let's visualize and compare the performance of different strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame for easy analysis\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': r['strategy'],\n",
    "        'Accuracy': r['accuracy'],\n",
    "        'F1-Macro': r['f1_macro'],\n",
    "        'F1-Weighted': r['f1_weighted']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# Sort by F1-Macro score\n",
    "results_df = results_df.sort_values('F1-Macro', ascending=False)\n",
    "\n",
    "print(\"üìä Performance Ranking (by F1-Macro):\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "bars1 = axes[0].bar(range(len(results_df)), results_df['Accuracy'], alpha=0.7)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xticks(range(len(results_df)))\n",
    "axes[0].set_xticklabels(results_df['Strategy'], rotation=45, ha='right')\n",
    "axes[0].set_ylim(0, 1)\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: F1-Macro comparison\n",
    "bars2 = axes[1].bar(range(len(results_df)), results_df['F1-Macro'], alpha=0.7, color='orange')\n",
    "axes[1].set_title('F1-Macro Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('F1-Macro Score')\n",
    "axes[1].set_xticks(range(len(results_df)))\n",
    "axes[1].set_xticklabels(results_df['Strategy'], rotation=45, ha='right')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(results_df['F1-Macro']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Improvement over baseline\n",
    "baseline_f1 = results_df[results_df['Strategy'].str.contains('Baseline')]['F1-Macro'].iloc[0]\n",
    "improvements = (results_df['F1-Macro'] - baseline_f1) / baseline_f1 * 100\n",
    "colors = ['red' if x < 0 else 'green' for x in improvements]\n",
    "bars3 = axes[2].bar(range(len(results_df)), improvements, alpha=0.7, color=colors)\n",
    "axes[2].set_title('Improvement over Baseline (%)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Improvement (%)')\n",
    "axes[2].set_xticks(range(len(results_df)))\n",
    "axes[2].set_xticklabels(results_df['Strategy'], rotation=45, ha='right')\n",
    "axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "for i, v in enumerate(improvements):\n",
    "    axes[2].text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best strategy\n",
    "best_strategy = results_df.iloc[0]\n",
    "print(f\"\\nüèÜ Best Strategy: {best_strategy['Strategy']}\")\n",
    "print(f\"   F1-Macro: {best_strategy['F1-Macro']:.3f}\")\n",
    "print(f\"   Improvement over baseline: {improvements.iloc[0]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Progress Analysis\n",
    "\n",
    "Let's examine how different strategies learn over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for top strategies\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Select interesting strategies to plot\n",
    "strategies_to_plot = [\n",
    "    'Confidence-0.95 + AppendGrow',\n",
    "    'Confidence-0.90 + AppendGrow', \n",
    "    'TopK-10 + AppendGrow',\n",
    "    'TopK-5 + AppendGrow'\n",
    "]\n",
    "\n",
    "# Plot 1: Number of labeled samples over iterations\n",
    "plt.subplot(2, 2, 1)\n",
    "for result in results:\n",
    "    if result['strategy'] in strategies_to_plot and result['history']:\n",
    "        iterations = [h['iteration'] for h in result['history']]\n",
    "        labeled_counts = [h['labeled_data_count'] for h in result['history']]\n",
    "        plt.plot(iterations, labeled_counts, marker='o', label=result['strategy'])\n",
    "\n",
    "plt.title('Labeled Data Growth', fontweight='bold')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Number of Labeled Samples')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: New labels added per iteration\n",
    "plt.subplot(2, 2, 2)\n",
    "for result in results:\n",
    "    if result['strategy'] in strategies_to_plot and result['history']:\n",
    "        iterations = [h['iteration'] for h in result['history']]\n",
    "        new_labels = [h['new_labels_count'] for h in result['history']]\n",
    "        plt.plot(iterations, new_labels, marker='s', label=result['strategy'])\n",
    "\n",
    "plt.title('New Labels Added per Iteration', fontweight='bold')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('New Labels Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Average confidence over iterations\n",
    "plt.subplot(2, 2, 3)\n",
    "for result in results:\n",
    "    if result['strategy'] in strategies_to_plot and result['history']:\n",
    "        iterations = [h['iteration'] for h in result['history']]\n",
    "        avg_confidence = [h['average_confidence'] for h in result['history']]\n",
    "        plt.plot(iterations, avg_confidence, marker='^', label=result['strategy'])\n",
    "\n",
    "plt.title('Average Confidence of New Labels', fontweight='bold')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average Confidence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Validation score progression (if available)\n",
    "plt.subplot(2, 2, 4)\n",
    "for result in results:\n",
    "    if result['strategy'] in strategies_to_plot and result['history']:\n",
    "        iterations = [h['iteration'] for h in result['history']]\n",
    "        val_scores = [h.get('validation_score', None) for h in result['history']]\n",
    "        # Only plot if we have validation scores\n",
    "        if any(v is not None for v in val_scores):\n",
    "            val_scores = [v if v is not None else 0 for v in val_scores]\n",
    "            plt.plot(iterations, val_scores, marker='d', label=result['strategy'])\n",
    "\n",
    "plt.title('Validation Score Progression', fontweight='bold')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Validation Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix Analysis\n",
    "\n",
    "Let's see how well each strategy handles the imbalanced classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confusion matrices for top strategies\n",
    "strategies_to_analyze = [\n",
    "    'Baseline (Supervised)',\n",
    "    best_strategy['Strategy'],  # Best SSL strategy\n",
    "    'TopK-10 + AppendGrow',     # Popular TopK strategy\n",
    "    'Confidence-0.90 + AppendGrow'  # Popular Confidence strategy\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, strategy_name in enumerate(strategies_to_analyze):\n",
    "    # Find the result for this strategy\n",
    "    result = next(r for r in results if r['strategy'] == strategy_name)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                xticklabels=[f'Class {j}' for j in range(3)],\n",
    "                yticklabels=[f'Class {j}' for j in range(3)])\n",
    "    \n",
    "    axes[i].set_title(f'{strategy_name}\\nF1-Macro: {result[\"f1_macro\"]:.3f}', fontweight='bold')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class performance analysis\n",
    "print(\"\\nüìä Per-Class Performance Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for strategy_name in strategies_to_analyze:\n",
    "    result = next(r for r in results if r['strategy'] == strategy_name)\n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    print(classification_report(y_test, result['predictions'], \n",
    "                              target_names=[f'Class {i}' for i in range(3)],\n",
    "                              digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Strategy Insights & Recommendations\n",
    "\n",
    "Based on our experiments, let's derive insights about when to use each strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze strategy characteristics\n",
    "strategy_analysis = []\n",
    "\n",
    "for result in results:\n",
    "    if result['strategy'] != 'Baseline (Supervised)' and result['history']:\n",
    "        total_iterations = len(result['history'])\n",
    "        total_labels_added = sum(h['new_labels_count'] for h in result['history'])\n",
    "        avg_confidence = np.mean([h['average_confidence'] for h in result['history']])\n",
    "        \n",
    "        strategy_analysis.append({\n",
    "            'Strategy': result['strategy'],\n",
    "            'F1-Macro': result['f1_macro'],\n",
    "            'Total Iterations': total_iterations,\n",
    "            'Labels Added': total_labels_added,\n",
    "            'Avg Confidence': avg_confidence,\n",
    "            'Stopping Reason': result['stopping_reason']\n",
    "        })\n",
    "\n",
    "analysis_df = pd.DataFrame(strategy_analysis)\n",
    "print(\"üîç Strategy Characteristics:\")\n",
    "print(analysis_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights & Recommendations\n",
    "\n",
    "Based on our comprehensive comparison, here are the key insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "baseline_f1 = results_df[results_df['Strategy'].str.contains('Baseline')]['F1-Macro'].iloc[0]\n",
    "best_ssl_f1 = results_df[~results_df['Strategy'].str.contains('Baseline')]['F1-Macro'].max()\n",
    "improvement = (best_ssl_f1 - baseline_f1) / baseline_f1 * 100\n",
    "\n",
    "print(\"üéØ KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìà Overall SSL Performance:\")\n",
    "print(f\"   ‚Ä¢ Best SSL improvement: +{improvement:.1f}% over baseline\")\n",
    "print(f\"   ‚Ä¢ Baseline F1-Macro: {baseline_f1:.3f}\")\n",
    "print(f\"   ‚Ä¢ Best SSL F1-Macro: {best_ssl_f1:.3f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Strategy: {best_strategy['Strategy']}\")\n",
    "\n",
    "print(f\"\\nüí° Strategy Recommendations:\")\n",
    "\n",
    "# Confidence Threshold analysis\n",
    "conf_strategies = [r for r in results if 'Confidence' in r['strategy'] and 'AppendGrow' in r['strategy']]\n",
    "if conf_strategies:\n",
    "    best_conf = max(conf_strategies, key=lambda x: x['f1_macro'])\n",
    "    print(f\"\\n   üéØ Confidence Threshold:\")\n",
    "    print(f\"      ‚Ä¢ Best threshold: {best_conf['strategy']}\")\n",
    "    print(f\"      ‚Ä¢ Use when: You want high-quality pseudo-labels\")\n",
    "    print(f\"      ‚Ä¢ Trade-off: Conservative (fewer labels, higher quality)\")\n",
    "\n",
    "# TopK analysis\n",
    "topk_strategies = [r for r in results if 'TopK' in r['strategy'] and 'AppendGrow' in r['strategy']]\n",
    "if topk_strategies:\n",
    "    best_topk = max(topk_strategies, key=lambda x: x['f1_macro'])\n",
    "    print(f\"\\n   üìä TopK Fixed Count:\")\n",
    "    print(f\"      ‚Ä¢ Best K value: {best_topk['strategy']}\")\n",
    "    print(f\"      ‚Ä¢ Use when: You want consistent progress each iteration\")\n",
    "    print(f\"      ‚Ä¢ Trade-off: Aggressive (more labels, variable quality)\")\n",
    "\n",
    "# Integration strategy analysis\n",
    "print(f\"\\n   üîÑ Integration Strategies:\")\n",
    "print(f\"      ‚Ä¢ AppendAndGrow: Best for most cases (monotonic growth)\")\n",
    "print(f\"      ‚Ä¢ FullReLabeling: Use when early pseudo-labels might be wrong\")\n",
    "print(f\"      ‚Ä¢ ConfidenceWeighting: Use with noisy pseudo-labels\")\n",
    "\n",
    "print(f\"\\nüé™ Imbalanced Data Insights:\")\n",
    "print(f\"   ‚Ä¢ SSL helps most with minority classes\")\n",
    "print(f\"   ‚Ä¢ Confidence-based strategies may bias toward majority class\")\n",
    "print(f\"   ‚Ä¢ TopK strategies provide more balanced pseudo-labeling\")\n",
    "\n",
    "print(f\"\\n‚ö° Quick Selection Guide:\")\n",
    "print(f\"   ‚Ä¢ Conservative approach: ConfidenceThreshold(0.95)\")\n",
    "print(f\"   ‚Ä¢ Balanced approach: ConfidenceThreshold(0.90)\")\n",
    "print(f\"   ‚Ä¢ Aggressive approach: TopKFixedCount(10-15)\")\n",
    "print(f\"   ‚Ä¢ Integration: AppendAndGrow() for most cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Now that you understand the different SSL strategies, here's how to apply this knowledge:\n",
    "\n",
    "### üéØ Strategy Selection Guide:\n",
    "\n",
    "**Use `ConfidenceThreshold`** when:\n",
    "- You prefer quality over quantity in pseudo-labels\n",
    "- Your base model is well-calibrated (probabilities are meaningful)\n",
    "- You have enough unlabeled data to be selective\n",
    "\n",
    "**Use `TopKFixedCount`** when:\n",
    "- You want predictable progress each iteration\n",
    "- You have limited unlabeled data\n",
    "- You're dealing with imbalanced classes\n",
    "\n",
    "**Integration strategies:**\n",
    "- `AppendAndGrow`: Default choice, works well in most cases\n",
    "- `FullReLabeling`: When you suspect early iterations produce poor pseudo-labels\n",
    "- `ConfidenceWeighting`: When you want to down-weight uncertain pseudo-labels\n",
    "\n",
    "### üîó Explore More:\n",
    "- **`03_text_classification.ipynb`** - Apply these strategies to NLP tasks\n",
    "- **`04_tabular_data_pipeline.ipynb`** - Integration with production pipelines\n",
    "- **`05_hyperparameter_tuning.ipynb`** - Optimize strategy parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}