{
 "cells": [
  {
   "cell_type": "code",
   "source": "# üöÄ Setup for Google Colab\nimport sys\nif 'google.colab' in sys.modules:\n    print(\"üîß Setting up for Google Colab...\")\n    \n    # Install required dependencies\n    !pip install -q matplotlib seaborn scikit-learn numpy pandas\n    \n    # Note: SSL framework code will be included in subsequent cells for Colab compatibility\n    print(\"‚úÖ Dependencies installed! SSL framework will be defined in the next cells.\")\nelse:\n    print(\"üìù Running locally - using installed SSL framework\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/pyssl/blob/main/notebooks/06_production_patterns.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Production Best Practices & Summary\n\n### üèóÔ∏è Key Production Patterns:\n\n1. **Configuration Management**: Use structured configs for all parameters\n2. **Model Versioning**: Track versions, metadata, and lineage  \n3. **Performance Monitoring**: Monitor latency, accuracy, and resource usage\n4. **Error Handling**: Robust exception handling and graceful degradation\n5. **Model Registry**: Centralized storage with metadata tracking\n6. **Validation Gates**: Ensure models meet requirements before deployment\n\n### üìã Production Deployment Checklist:\n\n**Pre-deployment:**\n- [ ] Model meets minimum performance thresholds\n- [ ] Latency requirements satisfied\n- [ ] Error handling tested\n- [ ] Configuration validated\n- [ ] Model registry updated\n\n**Post-deployment:**\n- [ ] Monitoring dashboard configured\n- [ ] Alerting rules established  \n- [ ] Rollback procedures tested\n- [ ] Performance baselines recorded\n- [ ] Continuous learning pipeline configured\n\nThis production framework provides enterprise-grade SSL deployment capabilities with proper monitoring, versioning, and lifecycle management.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create production configuration\nprod_config = ProductionConfig(\n    model_name=\"customer_sentiment_ssl\",\n    model_version=\"2.1.0\",\n    strategy_type=\"ConfidenceThreshold\",\n    strategy_params={\"threshold\": 0.93},\n    max_iterations=15,\n    min_accuracy=0.85,\n    max_prediction_latency_ms=50.0\n)\n\n# Initialize production service\nssl_service = SSLProductionService(prod_config)\n\n# Generate realistic production dataset\nprint(\"üìä Generating production dataset...\")\nX_labeled, y_labeled, X_unlabeled, X_val, y_val, X_test, y_test, y_unlabeled_true = generate_ssl_dataset(\n    dataset_type=\"classification\",\n    n_samples=5000,\n    n_labeled=100,  # Small labeled set - realistic for production\n    test_size=0.15,\n    val_size=0.10,\n    random_state=42,\n    n_features=15,\n    n_classes=3,\n    class_sep=0.75\n)\n\nprint(f\"   üìã Dataset: {len(X_labeled)} labeled, {len(X_unlabeled)} unlabeled\")\nprint(f\"   üéØ Classes: {len(np.unique(y_labeled))}\")\n\n# Train the production model\nsuccess = ssl_service.train_model(X_labeled, y_labeled, X_unlabeled, X_val, y_val)\n\nif success:\n    # Save the trained model\n    model_path = ssl_service.save_model()\n    \n    # Test predictions with performance monitoring\n    print(\"\\nüîÆ Testing production predictions...\")\n    predictions, confidences, latency = ssl_service.predict(X_test[:100], return_confidence=True)\n    \n    print(f\"   ‚ö° Prediction latency: {latency:.2f}ms per sample\")\n    print(f\"   üéØ Average confidence: {np.mean(confidences):.3f}\")\n    print(f\"   üìä Accuracy on test batch: {np.mean(predictions == y_test[:100]):.3f}\")\n    \n    # Performance check\n    from sklearn.metrics import accuracy_score, f1_score\n    all_predictions = ssl_service.predict(X_test)\n    test_accuracy = accuracy_score(y_test, all_predictions)\n    test_f1 = f1_score(y_test, all_predictions, average='macro')\n    \n    print(f\"\\nüìà Full Test Performance:\")\n    print(f\"   Accuracy: {test_accuracy:.3f} (min required: {prod_config.min_accuracy})\")\n    print(f\"   F1-Macro: {test_f1:.3f} (min required: {prod_config.min_f1_score})\")\n    \n    # Check if model meets production requirements\n    meets_requirements = (\n        test_accuracy >= prod_config.min_accuracy and \n        test_f1 >= prod_config.min_f1_score and\n        latency <= prod_config.max_prediction_latency_ms\n    )\n    \n    if meets_requirements:\n        print(\"‚úÖ Model meets all production requirements!\")\n    else:\n        print(\"‚ö†Ô∏è Model does not meet production requirements\")\n        \nelse:\n    print(\"‚ùå Training failed - cannot proceed to production\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Production Deployment Example\n\nLet's demonstrate the production service with a realistic example:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SSLProductionService:\n    \"\"\"Production-ready SSL service with monitoring and lifecycle management.\"\"\"\n    \n    def __init__(self, config: ProductionConfig):\n        self.config = config\n        self.model = None\n        self.model_metadata = {}\n        self.performance_history = []\n        self.prediction_cache = {}\n        \n        # Initialize directories\n        Path(config.model_registry_path).mkdir(exist_ok=True)\n        Path(config.metrics_storage_path).mkdir(exist_ok=True)\n        \n        print(f\"üöÄ SSL Production Service initialized: {config.model_name}\")\n    \n    def train_model(self, X_labeled: np.ndarray, y_labeled: np.ndarray, \n                   X_unlabeled: np.ndarray, X_val: np.ndarray = None, y_val: np.ndarray = None):\n        \"\"\"Train SSL model with production monitoring.\"\"\"\n        \n        print(f\"üîÑ Training {self.config.model_name} v{self.config.model_version}...\")\n        start_time = time.time()\n        \n        try:\n            # Create strategy\n            if self.config.strategy_type == \"ConfidenceThreshold\":\n                strategy = ConfidenceThreshold(**self.config.strategy_params)\n            elif self.config.strategy_type == \"TopKFixedCount\":\n                strategy = TopKFixedCount(**self.config.strategy_params)\n            else:\n                raise ValueError(f\"Unknown strategy: {self.config.strategy_type}\")\n            \n            # Create and train model\n            from sklearn.linear_model import LogisticRegression\n            base_model = LogisticRegression(random_state=42, max_iter=1000)\n            \n            self.model = SelfTrainingClassifier(\n                base_model=base_model,\n                selection_strategy=strategy,\n                integration_strategy=AppendAndGrow(),\n                max_iter=self.config.max_iterations,\n                labeling_convergence_threshold=self.config.convergence_threshold\n            )\n            \n            # Train with monitoring\n            self.model.fit(X_labeled, y_labeled, X_unlabeled, X_val, y_val)\n            \n            training_time = time.time() - start_time\n            \n            # Store metadata\n            self.model_metadata = {\n                'model_name': self.config.model_name,\n                'version': self.config.model_version,\n                'training_time_seconds': training_time,\n                'training_timestamp': datetime.now().isoformat(),\n                'initial_labeled_count': len(X_labeled),\n                'unlabeled_count': len(X_unlabeled),\n                'final_labeled_count': len(self.model.X_labeled_),\n                'strategy': self.config.strategy_type,\n                'strategy_params': self.config.strategy_params,\n                'iterations_completed': len(self.model.history_),\n                'stopping_reason': self.model.stopping_reason_\n            }\n            \n            print(f\"‚úÖ Training completed in {training_time:.2f}s\")\n            print(f\"   Final labeled samples: {self.model_metadata['final_labeled_count']}\")\n            print(f\"   Iterations: {self.model_metadata['iterations_completed']}\")\n            print(f\"   Stopping reason: {self.model_metadata['stopping_reason']}\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"‚ùå Training failed: {str(e)}\")\n            return False\n    \n    def predict(self, X: np.ndarray, return_confidence: bool = False) -> np.ndarray:\n        \"\"\"Make predictions with monitoring.\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not trained. Call train_model() first.\")\n        \n        start_time = time.time()\n        \n        try:\n            predictions = self.model.predict(X)\n            \n            if return_confidence:\n                probas = self.model.predict_proba(X)\n                confidences = np.max(probas, axis=1)\n                \n                prediction_time = time.time() - start_time\n                latency_ms = (prediction_time / len(X)) * 1000\n                \n                return predictions, confidences, latency_ms\n            else:\n                prediction_time = time.time() - start_time\n                latency_ms = (prediction_time / len(X)) * 1000\n                \n                # Log performance warning if latency is high\n                if latency_ms > self.config.max_prediction_latency_ms:\n                    print(f\"‚ö†Ô∏è High prediction latency: {latency_ms:.2f}ms per sample\")\n                \n                return predictions\n                \n        except Exception as e:\n            print(f\"‚ùå Prediction failed: {str(e)}\")\n            raise\n    \n    def save_model(self, filepath: str = None):\n        \"\"\"Save model and metadata to disk.\"\"\"\n        if filepath is None:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filepath = f\"{self.config.model_registry_path}/{self.config.model_name}_v{self.config.model_version}_{timestamp}.pkl\"\n        \n        try:\n            model_package = {\n                'model': self.model,\n                'metadata': self.model_metadata,\n                'config': self.config.to_dict()\n            }\n            \n            with open(filepath, 'wb') as f:\n                pickle.dump(model_package, f)\n            \n            print(f\"üíæ Model saved: {filepath}\")\n            return filepath\n            \n        except Exception as e:\n            print(f\"‚ùå Save failed: {str(e)}\")\n            return None\n    \n    def load_model(self, filepath: str):\n        \"\"\"Load model and metadata from disk.\"\"\"\n        try:\n            with open(filepath, 'rb') as f:\n                model_package = pickle.load(f)\n            \n            self.model = model_package['model'] \n            self.model_metadata = model_package['metadata']\n            \n            print(f\"üìÇ Model loaded: {self.model_metadata['model_name']} v{self.model_metadata['version']}\")\n            return True\n            \n        except Exception as e:\n            print(f\"‚ùå Load failed: {str(e)}\")\n            return False\n\nprint(\"‚úÖ SSL Production Service ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Production SSL Service\n\nNow let's create a production-ready SSL service with proper error handling, monitoring, and lifecycle management:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass ProductionConfig:\n    \"\"\"Production configuration for SSL systems.\"\"\"\n    \n    # Model Configuration\n    model_name: str = \"ssl_classifier_v1\"\n    model_version: str = \"1.0.0\"\n    strategy_type: str = \"ConfidenceThreshold\"\n    strategy_params: Dict = None\n    max_iterations: int = 10\n    convergence_threshold: int = 5\n    \n    # Data Configuration  \n    batch_size: int = 1000\n    retrain_threshold: float = 0.05  # Performance drop threshold\n    min_confidence_for_auto_label: float = 0.95\n    max_unlabeled_ratio: float = 0.8\n    \n    # Infrastructure\n    model_registry_path: str = \"./model_registry\"\n    metrics_storage_path: str = \"./metrics\"\n    log_level: str = \"INFO\"\n    enable_monitoring: bool = True\n    \n    # Performance Thresholds\n    min_accuracy: float = 0.8\n    min_f1_score: float = 0.75\n    max_prediction_latency_ms: float = 100.0\n    max_training_time_hours: float = 2.0\n    \n    def __post_init__(self):\n        if self.strategy_params is None:\n            if self.strategy_type == \"ConfidenceThreshold\":\n                self.strategy_params = {\"threshold\": 0.95}\n            elif self.strategy_type == \"TopKFixedCount\":\n                self.strategy_params = {\"k\": 10}\n    \n    def to_dict(self) -> Dict:\n        return asdict(self)\n    \n    @classmethod\n    def from_dict(cls, config_dict: Dict):\n        return cls(**config_dict)\n    \n    def save(self, filepath: str):\n        \"\"\"Save configuration to JSON file.\"\"\"\n        with open(filepath, 'w') as f:\n            json.dump(self.to_dict(), f, indent=2)\n    \n    @classmethod\n    def load(cls, filepath: str):\n        \"\"\"Load configuration from JSON file.\"\"\"\n        with open(filepath, 'r') as f:\n            config_dict = json.load(f)\n        return cls.from_dict(config_dict)\n\n# Example usage\nconfig = ProductionConfig(\n    model_name=\"customer_intent_ssl\",\n    strategy_type=\"ConfidenceThreshold\", \n    strategy_params={\"threshold\": 0.92},\n    min_accuracy=0.85\n)\n\nprint(\"üîß Production Configuration:\")\nprint(json.dumps(config.to_dict(), indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Production Configuration System\n\nFirst, let's create a robust configuration management system for production SSL deployments:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üöÄ SSL Production Patterns - Enterprise Deployment\n\nThis notebook demonstrates production-ready patterns for deploying semi-supervised learning systems at scale. We'll cover monitoring, versioning, continuous learning, and deployment best practices.\n\n**What you'll learn:**\n- Production SSL model lifecycle management\n- Monitoring and alerting for SSL systems\n- Continuous learning and model updating\n- Performance tracking and drift detection\n- Deployment patterns and infrastructure considerations\n- Error handling and reliability patterns\n\n**Focus:** Enterprise-grade SSL deployment patterns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport pickle\nimport json\nimport warnings\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nimport time\nwarnings.filterwarnings('ignore')\n\n# Import our SSL framework\nimport sys\nsys.path.append('../')\nfrom ssl_framework.main import SelfTrainingClassifier\nfrom ssl_framework.strategies import ConfidenceThreshold, TopKFixedCount, AppendAndGrow\n\n# Import utilities\nfrom utils.data_generation import generate_ssl_dataset\n\n# Set style\nplt.style.use('default')\nsns.set_palette(\"viridis\")\n\nprint(\"‚úÖ All imports successful!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}